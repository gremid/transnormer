# -*- coding = utf-8 -*-

# Select GPU
gpu = "cuda:0"  # when you did `export CUDA_VISIBLE_DEVICES=1` first

# Random seed (fix for reproducibility)
random_seed = 42

# Data
[data]
paths_train = [
    "data/interim/dtak-v03-1600-1699/dtak-v03-1600-1699-train.jsonl",
    # "data/interim/dtak-v03-1700-1799/dtak-v03-1700-1799-train.jsonl",
    # "data/interim/dtak-v03-1800-1899/dtak-v03-1800-1899-train.jsonl",
    ]

paths_validation = [
    "data/interim/dtak-v03-1600-1699/dtak-v03-1600-1699-validation.jsonl",
    # "data/interim/dtak-v03-1700-1799/dtak-v03-1700-1799-validation.jsonl",
    # "data/interim/dtak-v03-1800-1899/dtak-v03-1800-1899-validation.jsonl",
    ]

paths_test = [
    "data/interim/dtak-v03-1600-1699/dtak-v03-1600-1699-test.jsonl",
    # "data/interim/dtak-v03-1700-1799/dtak-v03-1700-1799-test.jsonl",
    # "data/interim/dtak-v03-1800-1899/dtak-v03-1800-1899-test.jsonl",
    ]
# n_examples_{split} must be the same length as paths_{split}; each element is
# an integer that defines the number of examples from the respective dataset
# split that will be used during training; setting it to a giant number like
# 1,000,000,000 is a simple way to ensure that all examples in this split will
# be used
n_examples_train = [
    10_000,
    # 1_000_000_000,
    # 1_000_000_000,
    ]
n_examples_validation = [
    1000,
    # 1_000_000_000,
    # 1_000_000_000,
    ]
n_examples_test = [
    1,
    # 1_000_000_000,
    # 1_000_000_000,
    ]

# # Data subsets sizes
# [subset_sizes]
# train = 100
# validation = 10
# test = 1

[tokenizer]
# path = "resources/tokenizer-wp-dta1600-1899"
# padding = true
# truncation = true
max_length_input = 256
max_length_output = 256
# input_transliterator = "Transliterator1"

# Model that is retrained
[language_models]
# checkpoint_encoder = "dbmdz/bert-base-historic-multilingual-cased"
# checkpoint_decoder = "bert-base-multilingual-cased"
checkpoint_encoder_decoder = "google/byt5-small"

[training_hyperparams]
batch_size = 32
epochs = 100
# learning_rate = 0.0005
logging_steps = 1_000
eval_steps = 1_000
eval_strategy = "steps"
save_steps = 15_000
fp16 = false


# Params for beam search decoding
# see https://huggingface.co/blog/how-to-generate and https://huggingface.co/transformers/v4.10.1/main_classes/model.html
# These initial parameters were copied from
# https://huggingface.co/blog/warm-starting-encoder-decoder#warm-starting-the-encoder-decoder-model
[beam_search_decoding]
no_repeat_ngram_size = 3
early_stopping = true
length_penalty = 2.0
num_beams = 4
