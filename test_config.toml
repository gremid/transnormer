# -*- coding = utf-8 -*-

# Select GPU
gpu = "cuda:0"  # when you did `export CUDA_VISIBLE_DEVICES=1` first

# Random seed (fix for reproducibility)
random_seed = 42

[data]
path_test = "/home/bracke/data/dta/jsonl/v09-lm/not-dtaec/1800-1899/dtak-test.jsonl"
n_examples_test = 200_000

[tokenizer]
checkpoint_in = "google/byt5-small"

[tokenizer_configs]
padding="longest" # pad to the longest sequence in batch
truncation=false
# max_length = None

[model]
checkpoint = "models/models_2024-08-22/checkpoint-612020"

[generation]
batch_size = 8

[generation_config]
# See here for more options: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig
max_new_tokens = 2048
num_beams = 4
early_stopping = false
length_penalty = 2.0
