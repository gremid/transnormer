{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data ...\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import glob\n",
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import tomli\n",
    "import tracemalloc\n",
    "from typing import Iterator, Dict, List, Generator, Tuple, Optional, Any, TextIO\n",
    "\n",
    "# import codecarbon\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers\n",
    "\n",
    "# TODO check these imports\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.normalizers import Normalizer\n",
    "from tokenizers import Tokenizer, Regex, NormalizedString, PreTokenizedString\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "def file_gen(path : str) -> Generator[TextIO, None, None]:\n",
    "    \"\"\" Yields file(s) from a path, where path can be file, dir or glob \"\"\"\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            yield file\n",
    "    elif os.path.isdir(path):\n",
    "        for filename in os.listdir(path):\n",
    "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
    "                yield file\n",
    "    else:\n",
    "        for filename in glob.glob(path):\n",
    "            with open(filename, 'r', encoding='utf-8') as file:\n",
    "                yield file\n",
    "\n",
    "def load_dtaeval_as_dataset(path:str) -> datasets.Dataset:\n",
    "    \"\"\" \n",
    "    Load the file(s) under `path` into a datasets.Dataset with columns \"orig\" and \"norm\"\n",
    "\n",
    "    If `path` is a directory name, \n",
    "    \"\"\"\n",
    "    \n",
    "    docs = [load_tsv(file, keep_sentences=True) for file in file_gen(path)]\n",
    "    docs_sent_joined = [[\n",
    "            [\" \".join(sent) for sent in column] for column in doc\n",
    "        ] for doc in docs]\n",
    "        \n",
    "    all_sents_orig, all_sents_norm = [], []\n",
    "    for doc_orig, doc_norm in docs_sent_joined:\n",
    "        all_sents_orig.extend([sent for sent in doc_orig])\n",
    "        all_sents_norm.extend([sent for sent in doc_norm])\n",
    "\n",
    "    return datasets.Dataset.from_dict({\"orig\" : all_sents_orig, \"norm\" : all_sents_norm})\n",
    "\n",
    "def load_tsv(file_obj, keep_sentences=True):\n",
    "    \"\"\"\n",
    "    Load a corpus in a tab-separated plain text file into lists\n",
    "\n",
    "    `keep_sentences` : if set to True, empty lines are interpreted\n",
    "    as sentence breaks. Consecutive empty lines are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    line = file_obj.readline()\n",
    "    # Read upto first non-empty line\n",
    "    while line.isspace():\n",
    "        line = f.readline()\n",
    "    # Number of columns in text file\n",
    "    n_columns = line.strip().count(\"\\t\") + 1\n",
    "    # Initial empty columns with one empty sentence inside\n",
    "    columns = [[[]] for i in range(n_columns)]\n",
    "    # Read file\n",
    "    line_cnt = 0\n",
    "    sent_cnt = 0\n",
    "    while line:\n",
    "        # non-empty line\n",
    "        if not line.isspace():\n",
    "            line = line.strip()\n",
    "            line_split = line.split(\"\\t\")\n",
    "\n",
    "            # Catch/skip ill-formed lines\n",
    "            if len(line_split) != n_columns:\n",
    "                print(\n",
    "                    f\"Line {line_cnt+1} does not have length \"\n",
    "                    f\"{n_columns} but {len(line_split)} skip line: '{line}'\"\n",
    "                )\n",
    "            else:\n",
    "                # build up sentences\n",
    "                for i in range(n_columns):\n",
    "                    columns[i][sent_cnt].append(line_split[i])\n",
    "\n",
    "        # empty line\n",
    "        else:\n",
    "            # current sentence empty?\n",
    "            # then just replace with empty sentence again\n",
    "            if columns[0][sent_cnt] == []:\n",
    "                for i in range(n_columns):\n",
    "                    columns[i][sent_cnt] = []\n",
    "            # else: move to build next sentence\n",
    "            else:\n",
    "                for i in range(n_columns):\n",
    "                    columns[i].append([])\n",
    "                sent_cnt += 1\n",
    "\n",
    "        # Move on\n",
    "        line = file_obj.readline()\n",
    "        line_cnt += 1\n",
    "\n",
    "    # optional: flatten structure\n",
    "    if not keep_sentences:\n",
    "        columns = [list(itertools.chain(*col)) for col in columns]\n",
    "\n",
    "    return columns\n",
    "\n",
    "def load_dtaeval_all() -> datasets.DatasetDict:\n",
    "\n",
    "    datadir = \"/home/bracke/data/dta/dtaeval/split-v3.1/txt\"\n",
    "\n",
    "    train_path = os.path.join(datadir, \"train\")\n",
    "    validation_path = os.path.join(datadir, \"dev\")\n",
    "    test_path = os.path.join(datadir, \"test\")\n",
    "\n",
    "    ds = datasets.DatasetDict()\n",
    "    ds[\"train\"] = load_dtaeval_as_dataset(train_path)\n",
    "    ds[\"validation\"] = load_dtaeval_as_dataset(validation_path)\n",
    "    ds[\"test\"] = load_dtaeval_as_dataset(test_path)\n",
    "\n",
    "    return ds\n",
    "\n",
    "class CustomNormalizer:\n",
    "    def normalize(self, normalized: NormalizedString):\n",
    "        normalized.nfd() # unicode decomposition\n",
    "        normalized.replace(\"ſ\", \"s\")\n",
    "        normalized.replace(\"ꝛ\", \"r\")\n",
    "        normalized.replace(chr(0x0303), \"\") # drop combining tilde\n",
    "        normalized.replace(chr(0x0364), \"e\")\n",
    "        normalized.replace(\"æ\", \"ae\")\n",
    "        normalized.replace(\"ů\",\"ü\")\n",
    "        normalized.replace(\"Ů\",\"Ü\")\n",
    "        normalized.nfc()\n",
    "\n",
    "\n",
    "# GPU set-up\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "################# Load tokenizers #######################\n",
    "tokenizer_hmbert_custom = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-historic-multilingual-cased\")\n",
    "tokenizer_hmbert_custom.backend_tokenizer.normalizer = Normalizer.custom(CustomNormalizer())\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "\n",
    "##################  Load data ###########################\n",
    "print(\"Loading the data ...\")\n",
    "\n",
    "dta_dataset = load_dtaeval_all()\n",
    "\n",
    "dta_dataset[\"validation\"] = (\n",
    "    dta_dataset[\"validation\"].shuffle().select(range(10)) # TODO\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?ba/s]/home/bracke/miniconda3/envs/gpu-venv-transnormer/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/home/bracke/miniconda3/envs/gpu-venv-transnormer/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 128 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.30s/ba]\n"
     ]
    }
   ],
   "source": [
    "# ################## Load model ##########\n",
    "checkpoint = \"models/models_2023-02-14_12-29/checkpoint-31000\"\n",
    "model = transformers.EncoderDecoderModel.from_pretrained(\n",
    "    checkpoint, \n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "def generate_normalization(batch):\n",
    "    inputs = tokenizer_hmbert_custom(batch[\"orig\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    output_str = tokenizer_bert.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # batch[\"norm_pred_ids\"] = outputs\n",
    "    batch[\"norm_pred_str\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "batch_size = 4  # change to 64 for full evaluation\n",
    "\n",
    "results = dta_dataset[\"validation\"].map(\n",
    "    generate_normalization, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig': 'Unaufhörlich rief er ſich jene Begebenheit zurück , welche einen unauslöſchlichen Eindruck auf ſein Gemüth gemacht hatte .', 'norm': 'Unaufhörlich rief er sich jene Begebenheit zurück , welche einen unauslöschlichen Eindruck auf sein Gemüt gemacht hatte .', 'norm_pred_str': 'Unaufhörlich rief er sich jene Begebenheit zurück, welche einen unauslöslichen Eindruck auf sein Gemüt gemacht hatte.'}\n"
     ]
    }
   ],
   "source": [
    "print(results[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ihm werden Licht und Feuer anflammen das Gesicht.', 'Der mit den Fröschen frech die Zedernbäume anschreit.', 'In einem Gespräch vorgestellt... Wie nämlich, durch Göttlichen Beistand eine wohlunterrichtete und geübte Wehe - Mutter ( Mit Verstand und geschickter Hand ( dergleichen verhüten ) oder wann Not ist, das Kind wenden könne', 'Gib mir bei Zeiten bescheid, daß es noch etwas zu küssen gibt.', 'Geht es noch?']\n"
     ]
    }
   ],
   "source": [
    "test_sents = [\"Jhm werden Licht und Feur anflammen das Geſicht.\",\n",
    "              \"Der mit den Froͤſchen frech die Cedern-Baͤum’ anſchreyt.\",\n",
    "              \"Jn einem Gespräch vorgestellet/ Wie nehmlich/ durch Göttlichen Beystand eine wohl-unterrichtete und geübte Wehe-Mutter/ Mit Verstand und geschickter Hand/ dergleichen verhüten/ oder wanns Noth ist/ das Kind wenden könne\",\n",
    "              \"Gib mir bey Zeiten bescheid, daß es noch etwas zu küßen gibt.\",\n",
    "              \"Gehts noch?\"\n",
    "]\n",
    "inputs = tokenizer_hmbert_custom(test_sents, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "# print(tokenizer_hmbert_custom.tokenize(\"Jhm werden Licht und Feur anflammen das Geſicht.\"))\n",
    "\n",
    "\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "attention_mask = inputs.attention_mask.to(device)\n",
    "outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=128)\n",
    "output_str = tokenizer_bert.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "print(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-venv-transnormer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78c4187baaf57098bb0b3703ce789bfcb46625a5d8666ee97b80d797f8c6f211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
